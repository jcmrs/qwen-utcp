{
  "repository": "utcp-agent",
  "commit_hash": "7501d17d6ea27ee1f29118c12beb12d6dea59d9e",
  "commit_timestamp": "1760610043",
  "file_count": 9,
  "extractions": [
    {
      "file_path": "UPSTREAM\\utcp-agent\\pyproject.toml",
      "content_type": "other",
      "content": "[build-system]\nrequires = [\"setuptools>=61.0\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"utcp-agent\"\nversion = \"1.0.2\"\ndescription = \"A ready-to-use agent for UTCP tool calling\"\nreadme = \"README.md\"\nlicense = \"MPL-2.0\"\nauthors = [\n    {name = \"UTCP Team\"}\n]\nmaintainers = [\n    {name = \"UTCP Team\"}\n]\nkeywords = [\"ai\", \"agent\", \"utcp\", \"langgraph\", \"langchain\", \"tools\"]\n\nclassifiers = [\n    \"Development Status :: 4 - Beta\",\n    \"Intended Audience :: Developers\",\n    \"Programming Language :: Python :: 3\",\n    \"Operating System :: OS Independent\",\n]\nrequires-python = \">=3.9\"\n\n# Core dependencies\ndependencies = [\n    \"langgraph>=0.0.40\",\n    \"langchain-core>=0.1.0\",\n    \"langchain>=0.1.0\",\n    \"pydantic>=2.0.0\",\n    \"utcp>=1.0.1\",\n    \"utcp-http>=1.0.3\",\n    \"utcp-mcp>=1.0.1\",\n    \"utcp-text>=1.0.1\",\n    \"utcp-cli>=1.0.1\",\n]\n\n[project.optional-dependencies]\n# LLM provider dependencies\nopenai = [\n    \"langchain-openai>=0.1.0\"\n]\nanthropic = [\n    \"langchain-anthropic>=0.1.0\"\n]\ncommunity = [\n    \"langchain-community>=0.0.20\"\n]\n\nexamples = [\n    \"python-dotenv>=1.0.0\",\n    \"langchain-openai>=0.1.0\",\n    \"langchain-anthropic>=0.1.0\",\n    \"langchain-community>=0.0.20\"\n]\n\n# Observability and monitoring\nobservability = [\n    \"langsmith>=0.1.0\"\n]\n\n# Development dependencies\ndev = [\n    \"build\",\n    \"pytest\",\n    \"pytest-asyncio\",\n    \"pytest-cov\",\n    \"coverage\",\n    \"twine\",\n]\n\n# All LLM providers\nllm-all = [\n    \"langchain-openai>=0.1.0\",\n    \"langchain-anthropic>=0.1.0\",\n    \"langchain-community>=0.0.20\"\n]\n\n# Everything for full functionality\nall = [\n    \"langchain-openai>=0.1.0\",\n    \"langchain-anthropic>=0.1.0\",\n    \"langchain-community>=0.0.20\",\n    \"langsmith>=0.1.0\"\n]\n\n[project.urls]\nHomepage = \"https://utcp.io\"\nSource = \"https://github.com/universal-tool-calling-protocol/utcp-agent\"\nIssues = \"https://github.com/universal-tool-calling-protocol/utcp-agent/issues\"",
      "line_count": 92,
      "word_count": 182,
      "title": "Pyproject.Toml",
      "summary": "[build-system] requires = [\"setuptools>=61.0\", \"wheel\"]",
      "key_terms": [
        "build",
        "project",
        "Python",
        "setuptools",
        "optional-dependencies",
        "md",
        "coverage",
        "twine",
        "langchain-anthropic",
        "dependencies",
        "providers",
        "community",
        "version",
        "pytest-asyncio",
        "llm",
        "https",
        "system",
        "Language",
        "utcp-mcp",
        "universal-tool"
      ],
      "timestamp": "2025-12-24T18:56:20.498988"
    },
    {
      "file_path": "UPSTREAM\\utcp-agent\\README.md",
      "content_type": "documentation",
      "content": "<div align=\"center\">\n<img alt=\"utcp agent logo\" src=\"https://github.com/user-attachments/assets/77723130-ecbc-4d1d-9e9b-20f978882699\" width=\"80%\" style=\"margin: 20px auto;\">\n\n<h1 align=\"center\">ðŸš€ Ready-to-use agent with intelligent tool-calling capabilities</h1>\n<p align=\"center\">\n    <a href=\"https://github.com/universal-tool-calling-protocol\">\n        <img src=\"https://img.shields.io/github/followers/universal-tool-calling-protocol?label=Follow%20Org&logo=github\" /></a>\n    <a href=\"https://pypi.org/project/utcp-agent/\" title=\"PyPI Version\">\n        <img src=\"https://img.shields.io/pypi/v/utcp-agent.svg\"/></a>\n    <a href=\"https://github.com/universal-tool-calling-protocol/utcp-agent/blob/main/LICENSE\" alt=\"License\">\n        <img src=\"https://img.shields.io/github/license/universal-tool-calling-protocol/utcp-agent\" /></a>\n    <a href=\"https://utcp.io/\" alt=\"Documentation\">\n        <img src=\"https://img.shields.io/badge/docs-utcp.io-blue\" /></a>\n</p>\n</div>\n\n**UTCP Agent** is the easiest way to build custom, ready-to-use agents which have intelligent tool calling capabilities and can connect to any native endpoint. The agent automatically discovers, searches, and executes UTCP tools based on user queries.\n\nThe Universal Tool Calling Protocol (UTCP) is an open standard that enables AI agents to discover and directly call tools across various communication protocols, eliminating the need for wrapper servers and reducing latency.\n\n## Features\n\n| Feature | Description |\n| :--- | :--- |\n| **ðŸ¤– Intelligent Tool Discovery** | Automatically searches and selects relevant UTCP tools based on user queries. |\n| **ðŸŒ Multi-LLM Support**| Compatible with OpenAI, Anthropic, and other LangChain-supported language models. |\n| **ðŸ”„ LangGraph Workflow** | Uses LangGraph for structured agent execution with proper state management. |\n| **ðŸ’¨ Streaming Support** | Optional streaming of workflow execution steps for real-time feedback. |\n| **ðŸ§  Conversation Memory** | Built-in conversation history and checkpointing for continuous conversations. |\n| **ðŸ”§ Flexible Configuration** | Easily configurable through UTCP client config and agent config. |\n\n## Quick Start\n\n### Installation\n\n```bash\npip install utcp-agent langchain-openai\n```\n\nSet your API key:\n```bash\nexport OPENAI_API_KEY=your_api_key_here\n```\n\n### Spin up your agent:\n\n```python\nimport asyncio\nimport os\nfrom langchain_openai import ChatOpenAI\nfrom utcp_agent import UtcpAgent\n\nasync def main():\n    # Set your OpenAI API key\n    llm = ChatOpenAI(\n        model=\"gpt-4o-mini\",\n        api_key=os.getenv(\"OPENAI_API_KEY\")\n    )\n    \n    # Create agent with book search capability\n    agent = await UtcpAgent.create(\n        llm=llm,\n        utcp_config={\n            \"manual_call_templates\": [{\n                \"name\": \"openlibrary\",\n                \"call_template_type\": \"http\",\n                \"http_method\": \"GET\",\n                \"url\": \"https://openlibrary.org/static/openapi.json\",\n                \"content_type\": \"application/json\"\n            }]\n        }\n    )\n    \n    # Chat with the agent\n    response = await agent.chat(\"Can you search for books by George Orwell?\")\n    print(f\"Agent: {response}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n## Advanced Configuration\n\n### With Memory and Custom Prompts\n\n```python\nfrom utcp_agent import UtcpAgent, UtcpAgentConfig\nfrom langgraph.checkpoint.memory import MemorySaver\n\nagent_config = UtcpAgentConfig(\n    max_tools_per_search=10,\n    checkpointer=MemorySaver(),\n    system_prompt=\"You are a helpful AI assistant with access to various tools through UTCP.\"\n)\n\nagent = await UtcpAgent.create(\n    llm=llm,\n    utcp_config=utcp_config,\n    agent_config=agent_config\n)\n\n# Use thread_id for conversation continuity\nresponse = await agent.chat(\"Find me a science fiction book\", thread_id=\"user_1\")\n```\n\n### With Environment Variables\n\n```python\nfrom pathlib import Path\n\nutcp_config = {\n    \"load_variables_from\": [{\n        \"variable_loader_type\": \"dotenv\",\n        \"env_file_path\": str(Path(__file__).parent / \".env\")\n    }],\n    \"manual_call_templates\": [{\n        \"name\": \"openlibrary\",\n        \"call_template_type\": \"http\", \n        \"http_method\": \"GET\",\n        \"url\": \"https://openlibrary.org/static/openapi.json\",\n        \"content_type\": \"application/json\"\n    }]\n}\n```\n\n### Streaming Execution\n\n```python\nasync for step in agent.stream(\"Search for AI books\"):\n    print(f\"Step: {step}\")\n```\n\n## Workflow\n\nThe agent follows a structured workflow using LangGraph, a library for building stateful, multi-actor applications with LLMs.\n\n1.  **Analyze Task**: Understands the user's query and formulates the current task.\n2.  **Search Tools**: Uses UTCP to find relevant tools for the task.\n3.  **Decide Action**: Determines whether to call tools or respond directly.\n4.  **Execute Tools**: Calls the selected tool with appropriate arguments.\n5.  **Respond**: Formats and returns the final response to the user.\n\n```mermaid\ngraph TD\n    A[User Input] --> B[Analyze Task]\n    B --> C[Search Tools]\n    C --> D{Decide Action}\n    D -->|Call Tool| F[Execute Tools]\n    D -->|Respond| G[Generate Response]\n    F --> B\n    G --> H[End]\n```\n\n## Examples\n\nSee the [`examples/` directory](https://github.com/universal-tool-calling-protocol/utcp-agent/tree/main/examples) for comprehensive examples:\n\n*   **`basic_openai.py`**: Using GPT models with book search.\n*   **`basic_anthropic.py`**: Using Claude models.\n*   **`streaming_example.py`**: Real-time workflow monitoring.\n*   **`config_file_example.py`**: Loading UTCP configuration from files.\n*   **`memory_conversation.py`**: Multi-turn conversations with memory.\n\n## Configuration Options\n\n### UtcpAgentConfig\n\n| Option | Description |\n| :--- | :--- |\n| `max_iterations` | Maximum workflow iterations (default: 3). |\n| `max_tools_per_search` | Maximum tools to retrieve per search (default: 10). |\n| `system_prompt` | Custom system prompt for the agent. |\n| `checkpointer`| LangGraph checkpointer for conversation memory. |\n| `callbacks` | LangChain callbacks for observability. |\n| `summarize_threshold` | Token count threshold for context summarization (default: 80000). |\n\n### UTCP Configuration\n\nThe agent accepts a standard UTCP client configuration, which can include:\n*   Variable definitions and loading\n*   Manual call templates\n*   Tool provider configurations\n\n## API Reference\n\n### UtcpAgent\n\n#### Class Methods\n\n*   `create(llm, utcp_config=None, agent_config=None, root_dir=None)`\n    *   Creates and initializes a UtcpAgent with an automatic UTCP client.\n\n#### Instance Methods\n\n*   `chat(user_input: str, thread_id: Optional[str] = None) -> str`\n    *   Processes user input and returns the agent's response.\n    *   Use `thread_id` for maintaining conversational continuity.\n\n*   `stream(user_input: str, thread_id: Optional[str] = None)`\n    *   Streams the workflow execution steps.\n\n## Error Handling\n\nThe agent includes comprehensive error handling to manage:\n*   Tool execution failures\n*   JSON parsing errors in LLM responses\n*   UTCP client errors\n*   Fallback responses to ensure the agent always provides a reply\n\n## Logging\n\nEnable logging to monitor the agent's behavior:\n\n```python\nimport logging\nlogging.basicConfig(level=logging.INFO)\n# Disable UTCP library logging for cleaner output\nlogging.getLogger(\"utcp\").setLevel(logging.WARNING)\n\n```\n\n## Contributing\n\n1. Follow the existing code style and patterns\n2. Add tests for new functionality\n3. Update documentation for API changes\n4. Ensure compatibility with UTCP core library\n\n## License\n\nSee LICENSE file for details.\n",
      "line_count": 232,
      "word_count": 847,
      "title": "Readme.Md",
      "summary": "<div align=\"center\"> <img alt=\"utcp agent logo\" src=\"https://github.com/user-attachments/assets/77723130-ecbc-4d1d-9e9b-20f978882699\" width=\"80%\" style=\"margin: 20px auto;\">",
      "key_terms": [
        "open",
        "comprehensive",
        "chat",
        "img",
        "monitor",
        "selected",
        "io-blue",
        "reply",
        "continuous",
        "LangChain-supported",
        "Creates",
        "model",
        "native",
        "if",
        "that",
        "Features",
        "Version",
        "supported",
        "formulates",
        "definitions"
      ],
      "timestamp": "2025-12-24T18:56:20.554337"
    },
    {
      "file_path": "UPSTREAM\\utcp-agent\\examples\\basic_anthropic.py",
      "content_type": "code",
      "content": "\"\"\"Anthropic Claude example for UtcpAgent.\n\nThis example demonstrates how to use UtcpAgent with Anthropic Claude models.\n\"\"\"\n\nimport asyncio\nimport logging\nimport os\nimport sys\nfrom pathlib import Path\n\ntry:\n    from langchain_anthropic import ChatAnthropic\nexcept ImportError:\n    print(\"Error: langchain_anthropic is required for this example.\")\n    print(\"Install with: pip install langchain-anthropic\")\n    sys.exit(1)\n\nfrom utcp_agent import UtcpAgent, UtcpAgentConfig\nfrom dotenv import load_dotenv\nfrom langgraph.checkpoint.memory import MemorySaver\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s [%(levelname)s] %(name)s:%(lineno)d - %(message)s',\n    handlers=[\n        logging.StreamHandler(sys.stdout)\n    ]\n)\n\n# Disable UTCP library logging\nlogging.getLogger(\"utcp\").setLevel(logging.WARNING)\n\nasync def main():\n    load_dotenv()\n    \"\"\"Example using Anthropic Claude models.\"\"\"\n    print(\"=== UtcpAgent with Anthropic ===\")\n    \n    # Check for API key\n    api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n    if not api_key:\n        print(\"Error: ANTHROPIC_API_KEY environment variable is required.\")\n        print(\"Set it with: export ANTHROPIC_API_KEY=your_key_here or in a .env file\")\n        return\n    \n    # Initialize Anthropic LLM\n    llm = ChatAnthropic(\n        model=\"claude-3-haiku-20240307\",\n        temperature=0.1,\n        api_key=api_key\n    )\n    \n    # Configure UTCP client (can load from config file or dict)\n    utcp_config = {\n        \"load_variables_from\": [\n            {\n                \"variable_loader_type\": \"dotenv\",\n                \"env_file_path\": str(Path(__file__).parent / \".env\")\n            }\n        ],\n        \"manual_call_templates\": [\n            {\n                \"name\": \"openlibrary\",\n                \"call_template_type\": \"http\",\n                \"http_method\": \"GET\",\n                \"url\": \"https://openlibrary.org/static/openapi.json\",\n                \"content_type\": \"application/json\"\n            }\n        ]\n    }\n    \n    # Configure agent\n    agent_config = UtcpAgentConfig(\n        max_tools_per_search=10,\n        checkpointer=MemorySaver(),\n        system_prompt=\"You are a helpful AI assistant with access to various tools through UTCP.\"\n    )\n    \n    # Create agent\n    try:\n        agent = await UtcpAgent.create(\n            llm=llm,\n            utcp_config=utcp_config,\n            agent_config=agent_config\n        )\n        \n        # Example conversations\n        print(\"Asking about books by a famous author...\")\n        response = await agent.chat(\"Can you find books by Isaac Asimov?\")\n        print(f\"Agent: {response}\\n\")\n        \n        print(\"Asking about a specific book...\")\n        response = await agent.chat(\"Tell me about the book 'Foundation' by Isaac Asimov\")\n        print(f\"Agent: {response}\\n\")\n        \n    except Exception as e:\n        print(f\"Error creating or using agent: {e}\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n",
      "line_count": 103,
      "word_count": 266,
      "title": "Basic Anthropic.Py",
      "summary": "\"\"\"Anthropic Claude example for UtcpAgent. This example demonstrates how to use UtcpAgent with Anthropic Claude models.",
      "key_terms": [
        "chat",
        "basicConfig",
        "async",
        "through",
        "format",
        "Utcp",
        "langchain-anthropic",
        "me",
        "ImportError",
        "Error",
        "AI",
        "except",
        "application",
        "You",
        "using",
        "llm",
        "model",
        "https",
        "if",
        "load"
      ],
      "timestamp": "2025-12-24T18:56:20.584608"
    },
    {
      "file_path": "UPSTREAM\\utcp-agent\\examples\\basic_openai.py",
      "content_type": "code",
      "content": "\"\"\"Basic OpenAI example for UtcpAgent.\n\nThis example demonstrates how to use UtcpAgent with OpenAI GPT models.\n\"\"\"\n\nimport asyncio\nimport logging\nimport os\nimport sys\nfrom pathlib import Path\ntry:\n    from langchain_openai import ChatOpenAI\nexcept ImportError:\n    print(\"Error: langchain_openai is required for this example.\")\n    print(\"Install with: pip install langchain_openai\")\n    sys.exit(1)\n\nfrom utcp_agent import UtcpAgent, UtcpAgentConfig\nfrom dotenv import load_dotenv\nfrom langgraph.checkpoint.memory import MemorySaver\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s [%(levelname)s] %(name)s:%(lineno)d - %(message)s',\n    handlers=[\n        logging.StreamHandler(sys.stdout)\n    ]\n)\n\n# Disable UTCP library logging\nlogging.getLogger(\"utcp\").setLevel(logging.WARNING)\n\nasync def main():\n    load_dotenv()\n    \"\"\"Example using OpenAI GPT models.\"\"\"\n    print(\"=== UtcpAgent with OpenAI ===\")\n    \n    # Check for API key\n    api_key = os.getenv(\"OPENAI_API_KEY\")\n    if not api_key:\n        print(\"Error: OPENAI_API_KEY environment variable is required.\")\n        print(\"Set it with: export OPENAI_API_KEY=your_key_here or in a .env file\")\n        return\n    \n    # Initialize OpenAI LLM\n    llm = ChatOpenAI(\n        model=\"gpt-4o-mini\",\n        temperature=0.1,\n        api_key=api_key\n    )\n    \n    # Configure UTCP client (can load from config file or dict)\n    utcp_config = {\n        \"load_variables_from\": [\n            {\n                \"variable_loader_type\": \"dotenv\",\n                \"env_file_path\": str(Path(__file__).parent / \".env\")\n            }\n        ],\n        \"manual_call_templates\": [\n            {\n                \"name\": \"openlibrary\",\n                \"call_template_type\": \"http\",\n                \"http_method\": \"GET\",\n                \"url\": \"https://openlibrary.org/static/openapi.json\",\n                \"content_type\": \"application/json\"\n            }\n        ]\n    }\n    \n    # Configure agent\n    agent_config = UtcpAgentConfig(\n        max_tools_per_search=10,\n        checkpointer=MemorySaver(),\n        system_prompt=\"You are a helpful AI assistant with access to various tools through UTCP.\"\n    )\n    \n    # Create agent\n    try:\n        agent = await UtcpAgent.create(\n            llm=llm,\n            utcp_config=utcp_config,\n            agent_config=agent_config\n        )\n        \n        # Example conversations\n        print(\"Asking about a book...\")\n        response = await agent.chat(\"Can you search for books by George Orwell?\")\n        print(f\"Agent: {response}\\n\")\n        \n        print(\"Asking about a specific book...\")\n        response = await agent.chat(\"Can you find information about the book '1984'?\")\n        print(f\"Agent: {response}\\n\")\n        \n    except Exception as e:\n        print(f\"Error creating or using agent: {e}\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n",
      "line_count": 102,
      "word_count": 263,
      "title": "Basic Openai.Py",
      "summary": "\"\"\"Basic OpenAI example for UtcpAgent. This example demonstrates how to use UtcpAgent with OpenAI GPT models.",
      "key_terms": [
        "chat",
        "basicConfig",
        "async",
        "search",
        "through",
        "format",
        "Utcp",
        "ImportError",
        "Error",
        "AI",
        "except",
        "application",
        "You",
        "using",
        "llm",
        "model",
        "https",
        "if",
        "load",
        "books"
      ],
      "timestamp": "2025-12-24T18:56:20.600646"
    },
    {
      "file_path": "UPSTREAM\\utcp-agent\\examples\\chat_interface_example.py",
      "content_type": "code",
      "content": "\"\"\"Basic OpenAI example for UtcpAgent.\n\nThis example demonstrates how to use UtcpAgent with OpenAI GPT models.\n\"\"\"\n\nimport asyncio\nimport logging\nimport os\nimport sys\nfrom pathlib import Path\ntry:\n    from langchain_openai import ChatOpenAI\nexcept ImportError:\n    print(\"Error: langchain_openai is required for this example.\")\n    print(\"Install with: pip install langchain_openai\")\n    sys.exit(1)\n\nfrom utcp_agent import UtcpAgent, UtcpAgentConfig\nfrom dotenv import load_dotenv\nfrom langgraph.checkpoint.memory import MemorySaver\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s [%(levelname)s] %(name)s:%(lineno)d - %(message)s',\n    handlers=[\n        logging.StreamHandler(sys.stdout)\n    ]\n)\n\n# Disable UTCP library logging\nlogging.getLogger(\"utcp\").setLevel(logging.WARNING)\n\nasync def main():\n    load_dotenv()\n    \"\"\"Example using OpenAI GPT models.\"\"\"\n    print(\"=== UtcpAgent with OpenAI ===\")\n    \n    # Initialize OpenAI LLM\n    llm = ChatOpenAI(\n        model=\"gpt-4o\",\n        api_key=os.getenv(\"OPENAI_API_KEY\")\n    )\n    \n    # Configure UTCP client (can load from config file or dict)\n    utcp_config = {\n        \"load_variables_from\": [\n            {\n                \"variable_loader_type\": \"dotenv\",\n                \"env_file_path\": \"./.env\"\n            }\n        ],\n        \"manual_call_templates\": [\n            {\n                \"name\": \"openlibrary\",\n                \"call_template_type\": \"http\",\n                \"http_method\": \"GET\",\n                \"url\": \"https://openlibrary.org/static/openapi.json\",\n                \"content_type\": \"application/json\"\n            },\n            {\n                \"name\": \"newsapi\",\n                \"call_template_type\": \"text\",\n                \"file_path\": \"./newsapi_manual.json\"\n            }\n        ]\n    }\n    \n    # Configure agent\n    agent_config = UtcpAgentConfig(\n        max_iterations=5,\n        max_tools_per_search=30,\n        system_prompt=\"You are a book and news AI assistant with access to the openlibrary and newsapi apis. Always try and use tools to accomplish your task, and always respond when your task is complete. Ask the user for more information if you need anything. If a variable requires to be set, ask the user to set the variable.\"\n    )\n    \n    # Create agent\n    try:\n        agent = await UtcpAgent.create(\n            llm=llm,\n            utcp_config=utcp_config,\n            agent_config=agent_config,\n            root_dir=str(Path(__file__).parent)\n        )\n        \n        while True:\n            user_input = input(\"User: \")\n            response = await agent.chat(user_input)\n            print(\"\\n\\n\\n\\n\")\n            print(f\"Agent: {response}\\n\")\n        \n    except Exception as e:\n        print(f\"Error creating or using agent: {e}\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n",
      "line_count": 97,
      "word_count": 260,
      "title": "Chat Interface Example.Py",
      "summary": "\"\"\"Basic OpenAI example for UtcpAgent. This example demonstrates how to use UtcpAgent with OpenAI GPT models.",
      "key_terms": [
        "chat",
        "basicConfig",
        "async",
        "If",
        "format",
        "Utcp",
        "ImportError",
        "Error",
        "AI",
        "except",
        "task",
        "application",
        "You",
        "using",
        "llm",
        "model",
        "https",
        "Ask",
        "if",
        "load"
      ],
      "timestamp": "2025-12-24T18:56:20.631818"
    },
    {
      "file_path": "UPSTREAM\\utcp-agent\\examples\\newsapi_manual.json",
      "content_type": "configuration",
      "content": "{\n  \"manual_version\": \"1.0\",\n  \"utcp_version\": \"1.0\",\n  \"tools\": [\n    {\n      \"name\": \"everything_get\",\n      \"description\": \"Search through millions of articles from over 150,000 large and small news sources and blogs. This endpoint suits article discovery and analysis. It requires either a search query, a source, or a domain.\",\n      \"tags\": [\n        \"articles\"\n      ],\n      \"inputs\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"q\": {\n            \"type\": \"string\",\n            \"description\": \"Keywords or phrases to search for in the article title and body. Advanced search is supported. Max length: 500 chars.\"\n          },\n          \"searchIn\": {\n            \"type\": \"string\",\n            \"description\": \"The fields to restrict your q search to. Possible options: title, description, content. Multiple options can be specified by separating them with a comma.\"\n          },\n          \"sources\": {\n            \"type\": \"string\",\n            \"description\": \"A comma-seperated string of identifiers (maximum 20) for the news sources or blogs you want headlines from.\"\n          },\n          \"domains\": {\n            \"type\": \"string\",\n            \"description\": \"A comma-seperated string of domains (eg bbc.co.uk, techcrunch.com, engadget.com) to restrict the search to.\"\n          },\n          \"excludeDomains\": {\n            \"type\": \"string\",\n            \"description\": \"A comma-seperated string of domains (eg bbc.co.uk, techcrunch.com, engadget.com) to remove from the results.\"\n          },\n          \"from\": {\n            \"type\": \"string\",\n            \"description\": \"A date and optional time for the oldest article allowed. This should be in ISO 8601 format (e.g. 2025-07-09 or 2025-07-09T09:28:11)\"\n          },\n          \"to\": {\n            \"type\": \"string\",\n            \"description\": \"A date and optional time for the newest article allowed. This should be in ISO 8601 format (e.g. 2025-07-09 or 2025-07-09T09:28:11)\"\n          },\n          \"language\": {\n            \"type\": \"string\",\n            \"description\": \"The 2-letter ISO-639-1 code of the language you want to get headlines for.\"\n          },\n          \"sortBy\": {\n            \"type\": \"string\",\n            \"description\": \"The order to sort the articles in. Possible options: relevancy, popularity, publishedAt.\"\n          },\n          \"pageSize\": {\n            \"type\": \"integer\",\n            \"description\": \"The number of results to return per page. Maximum: 100.\"\n          },\n          \"page\": {\n            \"type\": \"integer\",\n            \"description\": \"Use this to page through the results.\"\n          }\n        },\n        \"required\": [\n          \"q\"\n        ]\n      },\n      \"outputs\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"status\": {\n            \"type\": \"string\",\n            \"description\": \"If the request was successful or not. Options: ok, error.\"\n          },\n          \"totalResults\": {\n            \"type\": \"integer\",\n            \"description\": \"The total number of results available for your request.\"\n          },\n          \"articles\": {\n            \"type\": \"array\",\n            \"description\": \"The results of the request.\",\n            \"items\": {\n              \"type\": \"object\",\n              \"properties\": {\n                \"source\": {\n                  \"type\": \"object\",\n                  \"description\": \"The identifier id and a display name name for the source this article came from.\",\n                  \"properties\": {\n                    \"id\": {\n                      \"type\": \"string\"\n                    },\n                    \"name\": {\n                      \"type\": \"string\"\n                    }\n                  }\n                },\n                \"author\": {\n                  \"type\": \"string\",\n                  \"description\": \"The author of the article\"\n                },\n                \"title\": {\n                  \"type\": \"string\",\n                  \"description\": \"The headline or title of the article.\"\n                },\n                \"description\": {\n                  \"type\": \"string\",\n                  \"description\": \"A description or snippet from the article.\"\n                },\n                \"url\": {\n                  \"type\": \"string\",\n                  \"description\": \"The direct URL to the article.\"\n                },\n                \"urlToImage\": {\n                  \"type\": \"string\",\n                  \"description\": \"The URL to a relevant image for the article.\"\n                },\n                \"publishedAt\": {\n                  \"type\": \"string\",\n                  \"description\": \"The date and time that the article was published, in UTC (+000)\"\n                },\n                \"content\": {\n                  \"type\": \"string\",\n                  \"description\": \"The unformatted content of the article, where available. This is truncated to 200 chars.\"\n                }\n              },\n              \"required\": [\n                \"title\"\n              ]\n            }\n          }\n        }\n      },\n      \"tool_call_template\": {\n        \"call_template_type\": \"http\",\n        \"url\": \"https://newsapi.org/v2/everything\",\n        \"http_method\": \"GET\",\n        \"content_type\": \"application/json\",\n        \"auth\": {\n          \"auth_type\": \"api_key\",\n          \"api_key\": \"$NEWS_API_KEY\",\n          \"var_name\": \"X-Api-Key\"\n        }\n      }\n    },\n    {\n      \"name\": \"top_headlines_get\",\n      \"description\": \"This endpoint provides live top and breaking headlines for a country, specific category in a country, single source, or multiple sources. You can also search with keywords. Articles are sorted by the earliest date published first. This endpoint is great for retrieving headlines for use with news tickers or similar.\",\n      \"tags\": [\n        \"articles\"\n      ],\n      \"inputs\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"country\": {\n            \"type\": \"string\",\n            \"description\": \"The 2-letter ISO 3166-1 code of the country you want to get headlines for. Note: you can't mix this param with the sources param.\"\n          },\n          \"category\": {\n            \"type\": \"string\",\n            \"description\": \"The category you want to get headlines for. Possible options: business, entertainment, general, health, science, sports, technology. Note: you can't mix this param with the sources param.\"\n          },\n          \"sources\": {\n            \"type\": \"string\",\n            \"description\": \"A comma-seperated string of identifiers for the news sources or blogs you want headlines from. Note: you can't mix this param with the country or category params.\"\n          },\n          \"q\": {\n            \"type\": \"string\",\n            \"description\": \"Keywords or a phrase to search for.\"\n          },\n          \"pageSize\": {\n            \"type\": \"integer\",\n            \"description\": \"The number of results to return per page (request). 20 is the default, 100 is the maximum.\"\n          },\n          \"page\": {\n            \"type\": \"integer\",\n            \"description\": \"Use this to page through the results if the total results found is greater than the page size.\"\n          }\n        }\n      },\n      \"outputs\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"status\": {\n            \"type\": \"string\",\n            \"description\": \"If the request was successful or not. Options: ok, error.\"\n          },\n          \"totalResults\": {\n            \"type\": \"integer\",\n            \"description\": \"The total number of results available for your request.\"\n          },\n          \"articles\": {\n            \"type\": \"array\",\n            \"description\": \"The results of the request.\",\n            \"items\": {\n              \"type\": \"object\",\n              \"properties\": {\n                \"source\": {\n                  \"type\": \"object\",\n                  \"description\": \"The identifier id and a display name name for the source this article came from.\",\n                  \"properties\": {\n                    \"id\": {\n                      \"type\": \"string\"\n                    },\n                    \"name\": {\n                      \"type\": \"string\"\n                    }\n                  }\n                },\n                \"author\": {\n                  \"type\": \"string\",\n                  \"description\": \"The author of the article\"\n                },\n                \"title\": {\n                  \"type\": \"string\",\n                  \"description\": \"The headline or title of the article.\"\n                },\n                \"description\": {\n                  \"type\": \"string\",\n                  \"description\": \"A description or snippet from the article.\"\n                },\n                \"url\": {\n                  \"type\": \"string\",\n                  \"description\": \"The direct URL to the article.\"\n                },\n                \"urlToImage\": {\n                  \"type\": \"string\",\n                  \"description\": \"The URL to a relevant image for the article.\"\n                },\n                \"publishedAt\": {\n                  \"type\": \"string\",\n                  \"description\": \"The date and time that the article was published, in UTC (+000)\"\n                },\n                \"content\": {\n                  \"type\": \"string\",\n                  \"description\": \"The unformatted content of the article, where available. This is truncated to 200 chars.\"\n                }\n              },\n              \"required\": [\n                \"title\"\n              ]\n            }\n          }\n        }\n      },\n      \"tool_call_template\": {\n        \"call_template_type\": \"http\",\n        \"url\": \"https://newsapi.org/v2/top-headlines\",\n        \"http_method\": \"GET\",\n        \"content_type\": \"application/json\",\n        \"auth\": {\n          \"auth_type\": \"api_key\",\n          \"api_key\": \"$NEWS_API_KEY\",\n          \"var_name\": \"X-Api-Key\"\n        }\n      }\n    }\n  ]\n}",
      "line_count": 253,
      "word_count": 961,
      "title": "Newsapi Manual.Json",
      "summary": "\"manual_version\": \"1.0\", \"utcp_version\": \"1.0\",",
      "key_terms": [
        "ISO",
        "page",
        "phrase",
        "search",
        "number",
        "chars",
        "error",
        "co",
        "If",
        "display",
        "either",
        "through",
        "Articles",
        "id",
        "format",
        "available",
        "searchIn",
        "It",
        "specified",
        "small"
      ],
      "timestamp": "2025-12-24T18:56:20.654858"
    },
    {
      "file_path": "UPSTREAM\\utcp-agent\\examples\\streaming_example.py",
      "content_type": "code",
      "content": "\"\"\"Streaming workflow example for UtcpAgent.\n\nThis example demonstrates how to use UtcpAgent with streaming workflow execution.\n\"\"\"\n\nimport asyncio\nimport logging\nimport os\nimport sys\nfrom pathlib import Path\n\ntry:\n    from langchain_openai import ChatOpenAI\nexcept ImportError:\n    print(\"Error: langchain_openai is required for this example.\")\n    print(\"Install with: pip install langchain_openai\")\n    sys.exit(1)\n\nfrom utcp_agent import UtcpAgent, UtcpAgentConfig\nfrom dotenv import load_dotenv\nfrom langgraph.checkpoint.memory import MemorySaver\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s [%(levelname)s] %(name)s:%(lineno)d - %(message)s',\n    handlers=[\n        logging.StreamHandler(sys.stdout)\n    ]\n)\n\n# Disable UTCP library logging\nlogging.getLogger(\"utcp\").setLevel(logging.WARNING)\n\nasync def main():\n    load_dotenv()\n    \"\"\"Example using streaming workflow execution.\"\"\"\n    print(\"=== UtcpAgent with Streaming ===\")\n    \n    # Check for API key\n    api_key = os.getenv(\"OPENAI_API_KEY\")\n    if not api_key:\n        print(\"Error: OPENAI_API_KEY environment variable is required.\")\n        print(\"Set it with: export OPENAI_API_KEY=your_key_here or in a .env file\")\n        return\n    \n    llm = ChatOpenAI(\n        model=\"gpt-4o-mini\",\n        temperature=0.1,\n        api_key=api_key\n    )\n    \n    # Configure UTCP client\n    utcp_config = {\n        \"load_variables_from\": [\n            {\n                \"variable_loader_type\": \"dotenv\",\n                \"env_file_path\": str(Path(__file__).parent / \".env\")\n            }\n        ],\n        \"manual_call_templates\": [\n            {\n                \"name\": \"openlibrary\",\n                \"call_template_type\": \"http\",\n                \"http_method\": \"GET\",\n                \"url\": \"https://openlibrary.org/static/openapi.json\",\n                \"content_type\": \"application/json\"\n            }\n        ]\n    }\n    \n    # Configure agent\n    agent_config = UtcpAgentConfig(\n        max_tools_per_search=10,\n        checkpointer=MemorySaver(),\n        system_prompt=\"You are a helpful AI assistant with access to various tools through UTCP.\"\n    )\n    \n    try:\n        agent = await UtcpAgent.create(\n            llm=llm,\n            utcp_config=utcp_config,\n            agent_config=agent_config\n        )\n        \n        print(\"Streaming workflow execution:\")\n        print(\"=\" * 50)\n        \n        async for step in agent.stream(\"Can you find books about artificial intelligence?\"):\n            if \"error\" in step:\n                print(f\"âŒ Error: {step['error']}\")\n                break\n            else:\n                print(f\"ðŸ“‹ Step: {step}\")\n                print(\"-\" * 30)\n                \n    except Exception as e:\n        print(f\"Error creating or using agent: {e}\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n",
      "line_count": 103,
      "word_count": 243,
      "title": "Streaming Example.Py",
      "summary": "\"\"\"Streaming workflow example for UtcpAgent. This example demonstrates how to use UtcpAgent with streaming workflow execution.",
      "key_terms": [
        "basicConfig",
        "async",
        "error",
        "through",
        "format",
        "Utcp",
        "ImportError",
        "Error",
        "AI",
        "except",
        "application",
        "You",
        "using",
        "llm",
        "model",
        "https",
        "if",
        "books",
        "exit",
        "Configure"
      ],
      "timestamp": "2025-12-24T18:56:20.681765"
    },
    {
      "file_path": "UPSTREAM\\utcp-agent\\src\\utcp_agent\\utcp_agent.py",
      "content_type": "code",
      "content": "\"\"\"UtcpAgent: A LangGraph-based agent for UTCP tool calling.\n\nThis module provides a ready-to-use agent implementation that uses LangGraph\nto orchestrate UTCP tool discovery and execution. The agent follows a workflow\nsimilar to the original example but uses LangGraph for better state management\nand observability.\n\nWorkflow:\n1. User question\n2. Agent formulates task\n3. UTCP search tool using task\n4. Agent creates response (tool call or direct response)\n\"\"\"\n\nimport re\nimport json\nimport logging\nimport uuid\nfrom typing import Dict, Any, List, Optional, TypedDict, Annotated, Union\nfrom dataclasses import dataclass\nfrom pydantic import ValidationError\nfrom langchain_core.callbacks import Callbacks\nfrom langchain_core.language_models import BaseLanguageModel\nfrom langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage\nfrom langgraph.graph import StateGraph, END\nfrom langgraph.checkpoint.base import BaseCheckpointSaver\n\nfrom langgraph.graph.state import CompiledStateGraph, RunnableConfig\nfrom utcp.exceptions import UtcpVariableNotFound\nfrom utcp.utcp_client import UtcpClient\nfrom utcp.data.utcp_client_config import UtcpClientConfig\nfrom utcp.data.tool import Tool, ToolSerializer\n# Configure logger\nlogger = logging.getLogger(__name__)\n\n\nclass AgentState(TypedDict):\n    \"\"\"State for the UTCP agent workflow.\"\"\"\n    messages: Annotated[List[BaseMessage], \"The conversation messages\"]\n    current_task: str\n    available_tools: List[Dict[str, Any]]  # Serialized UTCP tool dictionaries\n    next_action: str  # \"search_tools\", \"call_tool\", \"respond\", \"end\"\n    decision_data: Optional[Dict[str, Any]]\n    iteration_count: int\n    final_response: Optional[str]\n\n\n@dataclass\nclass UtcpAgentConfig:\n    \"\"\"Configuration for the UTCP agent.\"\"\"\n    max_iterations: int = 3\n    max_tools_per_search: int = 10\n    system_prompt: Optional[str] = None\n    checkpointer: Optional[BaseCheckpointSaver] = None\n    callbacks: Optional[Callbacks] = None  # LangFuse, LangSmith, custom callbacks\n    summarize_threshold: int = 80000  # Token count threshold for context summarization\n    recursion_limit: int = 25\n\n\nclass UtcpAgent:\n    \"\"\"A LangGraph-based agent for UTCP tool calling.\n    \n    This agent uses LangGraph to orchestrate the workflow of:\n    1. Analyzing user queries to formulate tasks\n    2. Searching for relevant UTCP tools\n    3. Deciding whether to call tools or respond directly\n    4. Executing tool calls and processing results\n    \n    Attributes:\n        llm: The language model to use for decision making\n        utcp_client: The UTCP client for tool discovery and execution\n        config: Agent configuration\n        graph: The LangGraph StateGraph for workflow orchestration\n    \"\"\"\n    \n    def __init__(\n        self,\n        llm: BaseLanguageModel,\n        utcp_client: UtcpClient,\n        config: Optional[UtcpAgentConfig] = None,\n    ):\n        \"\"\"Initialize the UTCP agent.\n        \n        Args:\n            llm: LangChain language model for agent reasoning\n            utcp_client: UTCP client instance for tool operations\n            config: Optional agent configuration\n        \"\"\"\n        logger.info(\"Initializing UtcpAgent\")\n        self.llm = llm\n        self.utcp_client = utcp_client\n        self.config = config or UtcpAgentConfig()\n        \n        # Set up default system prompt\n        self.system_prompt = self.config.system_prompt or self._get_default_system_prompt()\n        \n        # Set up checkpointer before building graph\n        self.checkpointer = self.config.checkpointer\n        logger.info(f\"Checkpointer enabled: {self.checkpointer is not None}\")\n        \n        # Track current thread ID for conversation continuity\n        self.current_thread_id: Optional[str] = None\n        \n        # Build the LangGraph workflow\n        logger.info(\"Building LangGraph workflow\")\n        self.graph: CompiledStateGraph = self._build_graph()\n        logger.info(\"UtcpAgent initialization complete\")\n    \n    def _get_default_system_prompt(self) -> str:\n        \"\"\"Get the default system prompt for the agent.\"\"\"\n        return \"\"\"You are a helpful AI assistant with access to a wide variety of tools through UTCP.\n\nYour workflow:\n1. When given a user query, first analyze what task needs to be accomplished\n2. Search for relevant tools that can help with the task\n3. Either call appropriate tools or respond directly if no tools are needed\n4. Provide clear, helpful responses based on tool results or your knowledge\n\nYou have access to a special tool called 'respond' when you want to respond directly to the user without calling other tools.\n\nBe concise and helpful in your responses.\"\"\"\n\n    def _build_graph(self) -> CompiledStateGraph:\n        \"\"\"Build the LangGraph workflow.\"\"\"\n        workflow = StateGraph(AgentState)\n        \n        # Define workflow nodes\n        workflow.add_node(\"analyze_task\", self._analyze_task)\n        workflow.add_node(\"search_tools\", self._search_tools)\n        workflow.add_node(\"decide_action\", self._decide_action)\n        workflow.add_node(\"execute_tools\", self._execute_tools)\n        workflow.add_node(\"respond\", self._respond)\n        \n        # Define workflow edges\n        workflow.set_entry_point(\"analyze_task\")\n        \n        workflow.add_edge(\"analyze_task\", \"search_tools\")\n        workflow.add_edge(\"search_tools\", \"decide_action\")\n        \n        # Conditional routing from decide_action\n        workflow.add_conditional_edges(\n            \"decide_action\",\n            self._route_decision,\n            {\n                \"call_tool\": \"execute_tools\",\n                \"respond\": \"respond\",\n                \"end\": END,\n            }\n        )\n        \n        workflow.add_edge(\"execute_tools\", \"analyze_task\")  # Route back to decision making\n        workflow.add_edge(\"respond\", END)\n        \n        return workflow.compile(checkpointer=self.checkpointer)\n    \n    async def _analyze_task(self, state: AgentState) -> Dict[str, Any]:\n        \"\"\"Analyze the user query to formulate the current task.\"\"\"\n        messages = state[\"messages\"]\n        \n        # Create task analysis prompt\n        task_analysis_messages = [\n            SystemMessage(content=self.system_prompt),\n            SystemMessage(content=\"Based on the conversation history, what is the next step that needs to be accomplished? Respond with a concise next step description. Do not include 'the next step is' just the next step description.\"),\n            *messages,\n            HumanMessage(content=\"The next step is:\\n\")\n        ]\n\n        estimated_tokens = self._estimate_token_count(task_analysis_messages)\n        if estimated_tokens > self.config.summarize_threshold:\n            messages = await self._summarize_context(messages)\n            task_analysis_messages = [\n                SystemMessage(content=\"Based on the conversation history, what is the next step that needs to be accomplished? Respond with a concise next step description. Do not include 'the next step is' just the next step description.\"),\n                *messages,\n                HumanMessage(content=\"The next step is:\\n\")\n            ]\n        \n        try:\n            response = await self.llm.ainvoke(task_analysis_messages)\n            current_task = response.content.strip()\n            \n            logger.info(f\"[AnalyzeTask] Analyzed task: {current_task}\")\n            \n            messages = messages + [HumanMessage(content=\"The next step is:\\n\"), AIMessage(content=current_task)]\n\n            return {\n                \"current_task\": current_task,\n                \"next_action\": \"search_tools\",\n                \"messages\": messages\n            }\n        except Exception as e:\n            logger.error(f\"[AnalyzeTask] Error analyzing task: {e}\")\n            return {\n                \"current_task\": \"Unknown task\",\n                \"next_action\": \"respond\",\n                \"messages\": messages\n            }\n    \n    async def _search_tools(self, state: AgentState) -> Dict[str, Any]:\n        \"\"\"Search for relevant UTCP tools based on the current task.\"\"\"\n        current_task = state[\"current_task\"]\n        \n        try:\n            # Search for relevant tools using UTCP\n            logger.info(f\"[SearchTools] Searching for tools for task: {current_task}\")\n            \n            tools = await self.utcp_client.search_tools(current_task, limit=self.config.max_tools_per_search)\n            \n            logger.info(f\"[SearchTools] Found {len(tools)} relevant tools\")\n            for tool in tools:\n                logger.debug(f\"- {tool.name}: {tool.description}\")\n                \n            return {\n                \"available_tools\": [ToolSerializer().to_dict(tool) for tool in tools],\n                \"next_action\": \"decide_action\"\n            }\n            \n        except Exception as e:\n            logger.error(f\"[SearchTools] Error searching for tools: {e}\")\n            return {\n                \"available_tools\": [],\n                \"next_action\": \"decide_action\"\n            }\n    \n    async def _decide_action(self, state: AgentState) -> Dict[str, Any]:\n        \"\"\"Decide whether to call tools or respond directly.\"\"\"\n        messages = state[\"messages\"]\n        available_tools = state[\"available_tools\"]\n        current_task = state[\"current_task\"]\n        iteration_count = state.get(\"iteration_count\", 0)\n        \n        # Check iteration limit\n        if iteration_count >= self.config.max_iterations:\n            logger.info(f\"[DecideAction] Reached max iterations ({self.config.max_iterations}), responding\")\n            return {\n                \"next_action\": \"respond\",\n                \"decision_data\": {\"action\": \"respond\", \"message\": \"I've reached the maximum number of iterations. Let me provide a response based on what I've gathered so far.\"}\n            }\n        \n        # Increment iteration count\n        new_iteration_count = iteration_count + 1\n        logger.info(f\"[DecideAction] Iteration {new_iteration_count}/{self.config.max_iterations}\")\n        \n        # Prepare tool descriptions for the prompt\n        if available_tools:\n            tools_text = json.dumps([{\n                \"name\": tool[\"name\"],\n                \"description\": tool[\"description\"],\n                \"inputs\": tool[\"inputs\"]\n            } for tool in available_tools], indent=2)\n            logger.info(f\"[DecideAction] Evaluating {len(available_tools)} available tools for decision\")\n        else:\n            tools_text = \"No tools available\"\n            logger.info(\"[DecideAction] No tools available for decision\")\n        \n        # Create decision prompt\n        decision_prompt = f\"\"\"Given the current task: \"{current_task}\"\n\nAvailable tools:\n{tools_text}\n\nBased on the conversation and available tools, decide what to do next:\n1. If you need to use a tool to accomplish the task, respond with: {{\"action\": \"call_tool\", \"tool_name\": \"tool.name\", \"arguments\": {{\"arg1\": \"value1\"}}}}\n2. If you can answer directly without tools, respond with: {{\"action\": \"respond\", \"message\": \"your direct response\"}}\n3. If the conversation is complete, respond with: {{\"action\": \"end\"}}\n\nRespond ONLY with the JSON object, no other text.\"\"\"\n        \n        decision_messages = [\n            SystemMessage(content=self.system_prompt),\n            *messages,\n            HumanMessage(content=decision_prompt)\n        ]\n\n        estimated_tokens = self._estimate_token_count(decision_messages)\n\n        if estimated_tokens > self.config.summarize_threshold:\n            messages = await self._summarize_context(messages)\n            decision_messages = [\n                SystemMessage(content=self.system_prompt),\n                *messages,\n                HumanMessage(content=decision_prompt)\n            ]\n        \n        try:\n            response = await self.llm.ainvoke(decision_messages)\n            decision_text = response.content.strip()\n            \n            # Parse the JSON response\n            try:\n                json_match = re.search(r'```json\\n({.*?})\\n```', decision_text, re.DOTALL)\n                if not json_match:\n                    json_match = re.search(r'({.*})', decision_text, re.DOTALL)\n\n                decision = json.loads(json_match.group(1))\n                action = decision.get(\"action\", \"respond\")\n                \n                logger.info(f\"[DecideAction] Agent decision: {action}\")\n                if action == \"call_tool\":\n                    tool_name = decision.get(\"tool_name\", \"unknown\")\n                    logger.info(f\"[DecideAction] Selected tool: {tool_name}\")\n                \n                return {\n                    \"next_action\": action,\n                    \"decision_data\": decision,\n                    \"iteration_count\": new_iteration_count,\n                    \"messages\": messages\n                }\n            except json.JSONDecodeError:\n                logger.warning(f\"[DecideAction] Could not parse decision JSON: {decision_text}\")\n                return {\n                    \"next_action\": \"respond\",\n                    \"decision_data\": {\"action\": \"respond\", \"message\": decision_text},\n                    \"iteration_count\": new_iteration_count,\n                    \"messages\": messages\n                }\n                \n        except Exception as e:\n            logger.error(f\"[DecideAction] Error making decision: {e}\")\n            return {\n                \"next_action\": \"respond\",\n                \"decision_data\": {\"action\": \"respond\", \"message\": f\"I encountered an error: {str(e)}\"},\n                \"iteration_count\": new_iteration_count,\n                \"messages\": messages\n            }\n    \n    def _route_decision(self, state: AgentState) -> str:\n        \"\"\"Route based on the agent's decision.\"\"\"\n        next_action = state.get(\"next_action\", \"respond\")\n        logger.info(f\"[RouteDecision] Routing to: {next_action}\")\n        return next_action\n    \n    async def _execute_tools(self, state: AgentState) -> Dict[str, Any]:\n        \"\"\"Execute the selected tool.\"\"\"\n        decision_data = state.get(\"decision_data\", {})\n        messages = state[\"messages\"]\n        \n        tool_name = decision_data.get(\"tool_name\")\n        arguments = decision_data.get(\"arguments\", {})\n        \n        if not tool_name:\n            logger.error(\"[ExecuteTools] No tool name specified in decision\")\n            # Add error message to conversation\n            error_msg = \"Error: No tool specified\"\n            updated_messages = messages + [AIMessage(content=error_msg)]\n            return {\n                \"messages\": updated_messages,\n                \"next_action\": \"respond\"\n            }\n        \n        try:\n            logger.info(f\"[ExecuteTools] Executing tool: {tool_name} with arguments: {arguments}\")\n            \n            # Execute the tool\n            try:\n                result = await self.utcp_client.call_tool(tool_name, arguments)\n            except UtcpVariableNotFound as e:\n                required_variables = await self.utcp_client.get_required_variables_for_registered_tool(tool_name)\n                error_msg = f\"Tool {tool_name} requires the following variables to be set: {required_variables}.\"\n                logger.error(\"[ExecuteTools] \" + error_msg)\n                # Add error to messages\n                updated_messages = messages + [AIMessage(content=error_msg)]\n                return {\n                    \"messages\": updated_messages,\n                    \"next_action\": \"respond\"\n                }\n\n            logger.info(f\"[ExecuteTools] Tool result: {str(result)[:100] + '...' if len(str(result)) > 100 else str(result)}\")\n            \n            # Add tool call and result to messages\n            tool_call_msg = AIMessage(content=f\"Tool called: {tool_name} with arguments: {arguments}\")\n            try:\n                tool_result_msg = HumanMessage.model_validate(result)\n            except (ValidationError, TypeError):\n                try:\n                    tool_result_msg = HumanMessage.model_validate([result])\n                except (ValidationError, TypeError):\n                    if str(result).strip() == \"\":\n                        tool_result_msg = HumanMessage(content=\"Result is empty. Try different arguments or a different tool.\")\n                    else:\n                        tool_result_msg = HumanMessage(content=f\"Tool result: {result}\")\n                        \n                        if self._estimate_token_count([tool_result_msg]) > self.config.summarize_threshold:\n                            tool_result_msg = HumanMessage(content=\"Result is too long to display. Try different arguments or a different tool. This is the beginning of the result: \" + str(result)[:100] + \"...\")\n\n            updated_messages = messages + [\n                tool_call_msg,\n                tool_result_msg\n            ]\n            \n            return {\n                \"messages\": updated_messages,\n                \"next_action\": \"analyze_task\"  # Go back to decision making\n            }\n            \n        except Exception as e:\n            error_msg = f\"Error executing tool {tool_name}: {str(e)}\"\n            logger.error(\"[ExecuteTools] \" + error_msg)\n            \n            # Add error to messages\n            updated_messages = messages + [AIMessage(content=error_msg)]\n            return {\n                \"messages\": updated_messages,\n                \"next_action\": \"respond\"\n            }\n    \n    async def _respond(self, state: AgentState) -> Dict[str, Any]:\n        \"\"\"Generate the final response to the user.\"\"\"\n        messages = state[\"messages\"]\n        decision_data = state.get(\"decision_data\", {})\n        current_task = state[\"current_task\"]\n        \n        logger.info(\"[Respond] Generating response based on conversation history\")\n        response_prompt = f\"\"\"Based on the conversation history and the task: \"{current_task}\", provide a helpful summary response to the user. \n\nIf tools were called and results obtained, summarize what was accomplished and provide the relevant information from the tool results.\nIf no tools were needed, provide a direct helpful response.\n\nBe concise and helpful.\"\"\"\n            \n        response_messages = [\n            SystemMessage(content=self.system_prompt),\n            *messages,\n            HumanMessage(content=response_prompt)\n        ]\n\n        estimated_tokens = self._estimate_token_count(response_messages)\n\n        if estimated_tokens > self.config.summarize_threshold:\n            messages = await self._summarize_context(messages)\n            response_messages = [\n                SystemMessage(content=self.system_prompt),\n                *messages,\n                HumanMessage(content=response_prompt)\n            ]\n        \n        try:\n            response = await self.llm.ainvoke(response_messages)\n            response_text = response.content.strip()\n        except Exception as e:\n            logger.error(f\"[Respond] Error generating response: {e}\")\n            response_text = f\"I encountered an error generating the response: {str(e)}\"\n        \n        # Add the response to messages\n        updated_messages = messages + [AIMessage(content=response_text)]\n        logger.info(f\"[Respond] Generated response: {response_text[:100]}{'...' if len(response_text) > 100 else ''}\")\n        \n        return {\n            \"messages\": updated_messages,\n            \"final_response\": response_text\n        }\n    \n    def _estimate_token_count(self, messages: List[BaseMessage]) -> int:\n        \"\"\"Estimate token count for messages (rough approximation).\"\"\"\n        total_chars = sum(len(msg.content) for msg in messages)\n        # Rough approximation: 1 token â‰ˆ 4 characters for English text\n        return total_chars // 4\n    \n    async def _summarize_context(self, messages: List[BaseMessage]) -> List[BaseMessage]:\n        \"\"\"Manage context size by summarizing history if needed.\"\"\"\n        # Keep the system message (if any) and recent messages\n        system_messages = [msg for msg in messages if isinstance(msg, SystemMessage)]\n        non_system_messages = [msg for msg in messages if not isinstance(msg, SystemMessage)]\n        \n        # Keep last few messages as-is and summarize the rest\n        recent_count = 2\n        if len(non_system_messages) <= recent_count:\n            return messages\n        \n        messages_to_summarize = non_system_messages[:-recent_count]\n        recent_messages = non_system_messages[-recent_count:]\n        \n        # Create summarization prompt\n        conversation_text = \"\\n\".join([\n            f\"{'Human' if isinstance(msg, HumanMessage) else 'Assistant'}: {msg.content}\"\n            for msg in messages_to_summarize\n        ])\n        \n        summarization_prompt = f\"\"\"Please summarize the following conversation history concisely, preserving key information, decisions made, and context:\n\n{conversation_text}\n\nProvide a concise summary that captures the essential points and context.\"\"\"\n        \n        try:\n            summary_response = await self.llm.ainvoke([HumanMessage(content=summarization_prompt)])\n            summary = summary_response.content.strip()\n            \n            # Create summarized history message\n            summary_message = HumanMessage(content=f\"Conversation summary: {summary}\")\n            \n            logger.info(f\"[SummarizeContext] Summarized {len(messages_to_summarize)} messages\")\n            \n            # Return: system messages + summary + recent messages\n            return system_messages + [summary_message] + recent_messages\n            \n        except Exception as e:\n            logger.error(f\"[SummarizeContext] Error summarizing history: {e}\")\n            # Fallback: just truncate old messages\n            return system_messages + recent_messages\n    \n    async def chat(self, user_input: str, thread_id: Optional[str] = None) -> str:\n        \"\"\"Process a user input and return the agent's response.\n        \n        Args:\n            user_input: The user's message\n            thread_id: Optional thread ID for conversation continuity\n            \n        Returns:\n            Dictionary containing:\n            - \"response\": The agent's response text\n            - \"thread_id\": The thread ID used for this conversation\n        \"\"\"\n        logger.info(f\"[Chat] Processing user input: {user_input[:100]}{'...' if len(user_input) > 100 else ''}\")\n        logger.info(f\"[Chat] Thread ID: {thread_id}\")\n        \n        try:\n            # Create initial state\n            initial_state = {\n                \"messages\": [HumanMessage(content=user_input)],\n                \"current_task\": \"\",\n                \"available_tools\": [],\n                \"next_action\": \"analyze_task\",\n                \"tool_result\": None,\n                \"iteration_count\": 0\n            }\n            \n            # Configure for checkpointing and callbacks\n            config = {}\n            if self.config.recursion_limit:\n                config[\"recursion_limit\"] = self.config.recursion_limit\n            if self.checkpointer:\n                # Auto-generate thread_id if not provided but checkpointer is configured\n                actual_thread_id = thread_id or str(uuid.uuid4())\n                self.current_thread_id = actual_thread_id  # Store for conversation continuity\n                config[\"configurable\"] = {\"thread_id\": actual_thread_id}\n                logger.info(f\"[Chat] Using thread ID: {actual_thread_id}\")\n            if self.config.callbacks:\n                config[\"callbacks\"] = self.config.callbacks\n                logger.info(\"[Chat] Callbacks configured\")\n            \n            # Run the workflow\n            result = await self.graph.ainvoke(initial_state, config=config)\n            \n            final_response = result.get(\"final_response\", \"I'm sorry, I couldn't process your request.\")\n            logger.info(\"[Chat] Workflow completed successfully.\")\n            \n            return final_response\n            \n        except Exception as e:\n            logger.error(f\"[Chat] Error in chat: {e}\")\n            return f\"I encountered an error: {str(e)}\"\n    \n    async def stream(self, user_input: str, thread_id: Optional[str] = None):\n        \"\"\"Stream the agent's workflow execution.\n        \n        Args:\n            user_input: The user's message\n            thread_id: Optional thread ID for conversation continuity\n            \n        Yields:\n            Workflow steps and updates\n        \"\"\"\n        \n        try:\n            # Create initial state\n            initial_state = {\n                \"messages\": [HumanMessage(content=user_input)],\n                \"current_task\": \"\",\n                \"available_tools\": [],\n                \"next_action\": \"analyze_task\",\n                \"tool_result\": None,\n                \"iteration_count\": 0\n            }\n            \n            # Configure for checkpointing and callbacks\n            config = {}\n            if self.checkpointer:\n                # Auto-generate thread_id if not provided but checkpointer is configured\n                actual_thread_id = thread_id or str(uuid.uuid4())\n                self.current_thread_id = actual_thread_id  # Store for conversation continuity\n                config[\"configurable\"] = {\"thread_id\": actual_thread_id}\n                logger.info(f\"[Chat] Using thread ID: {actual_thread_id}\")\n            if self.config.callbacks:\n                config[\"callbacks\"] = self.config.callbacks\n                logger.info(\"[Chat] Callbacks configured\")\n            \n            # Stream the workflow\n            async for step in self.graph.astream(initial_state, config=config):\n                yield step\n                \n        except Exception as e:\n            logger.error(f\"[Stream] Error in stream: {e}\")\n            yield {\"error\": str(e)}\n    \n    def get_current_thread_id(self) -> Optional[str]:\n        \"\"\"Get the current thread ID for conversation continuity.\n        \n        Returns:\n            The current thread ID if available, None otherwise\n        \"\"\"\n        return self.current_thread_id\n    \n    @classmethod\n    async def create(\n        cls,\n        llm: BaseLanguageModel,\n        utcp_config: Optional[Union[str, Dict[str, Any], UtcpClientConfig]] = None,\n        agent_config: Optional[UtcpAgentConfig] = None,\n        root_dir: Optional[str] = None,\n    ) -> \"UtcpAgent\":\n        \"\"\"Create a new UtcpAgent with automatic UTCP client initialization.\n        \n        Args:\n            llm: LangChain language model\n            utcp_config: UTCP client configuration\n            agent_config: Agent configuration\n            root_dir: Root directory for UTCP client\n            \n        Returns:\n            Initialized UtcpAgent instance\n        \"\"\"\n        # Create UTCP client\n        utcp_client = await UtcpClient.create(root_dir=root_dir, config=utcp_config)\n        \n        # Create agent\n        return cls(llm=llm, utcp_client=utcp_client, config=agent_config)\n",
      "line_count": 627,
      "word_count": 2252,
      "title": "Utcp Agent.Py",
      "summary": "\"\"\"UtcpAgent: A LangGraph-based agent for UTCP tool calling. This module provides a ready-to-use agent implementation that uses LangGraph",
      "key_terms": [
        "chat",
        "clear",
        "display",
        "Either",
        "selected",
        "generating",
        "yield",
        "logger",
        "Found",
        "Auto-generate",
        "orchestration",
        "model",
        "if",
        "that",
        "Processing",
        "Configure",
        "completed",
        "formulates",
        "await",
        "Returns"
      ],
      "timestamp": "2025-12-24T18:56:20.732663"
    },
    {
      "file_path": "UPSTREAM\\utcp-agent\\src\\utcp_agent\\__init__.py",
      "content_type": "code",
      "content": "\"\"\"UtcpAgent: A LangGraph-based agent for UTCP tool calling.\n\nThis package provides a ready-to-use agent implementation that uses LangGraph\nto orchestrate UTCP tool discovery and execution.\n\"\"\"\n\nfrom utcp_agent.utcp_agent import UtcpAgent, UtcpAgentConfig, AgentState\n\n__version__ = \"1.0.0\"\n__all__ = [\"UtcpAgent\", \"UtcpAgentConfig\", \"AgentState\"]\n",
      "line_count": 11,
      "word_count": 40,
      "title": "  Init  .Py",
      "summary": "\"\"\"UtcpAgent: A LangGraph-based agent for UTCP tool calling. This package provides a ready-to-use agent implementation that uses LangGraph",
      "key_terms": [
        "from",
        "discovery",
        "based",
        "utcp",
        "calling",
        "uses",
        "Utcp",
        "tool",
        "LangGraph",
        "and",
        "for",
        "package",
        "execution",
        "This",
        "UTCP",
        "ready-to",
        "LangGraph-based",
        "agent",
        "AgentState",
        "use"
      ],
      "timestamp": "2025-12-24T18:56:20.743673"
    }
  ],
  "timestamp": "2025-12-24T18:56:20.743673"
}